{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take 2: Adaboost with Weak Classifiers\n",
    "None of that bagging classifier bullshit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import guys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "# from AdaBoostClassifier import AdaBoostClassifier\n",
    "#%run AdaBoostWeak.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: \n",
      "  (2, 0)\t1\n",
      "  (5, 0)\t1\n",
      "  (8, 0)\t1\n",
      "  (9, 0)\t1\n",
      "  (11, 0)\t1\n",
      "  (12, 0)\t1\n",
      "  (15, 0)\t1\n",
      "  (19, 0)\t1\n",
      "  (34, 0)\t1\n",
      "  (42, 0)\t1\n",
      "  (54, 0)\t1\n",
      "  (56, 0)\t1\n",
      "  (65, 0)\t1\n",
      "  (67, 0)\t1\n",
      "  (68, 0)\t1\n",
      "  (93, 0)\t1\n",
      "  (95, 0)\t1\n",
      "  (114, 0)\t1\n",
      "  (117, 0)\t1\n",
      "  (120, 0)\t1\n",
      "  (121, 0)\t1\n",
      "  (123, 0)\t1\n",
      "  (134, 0)\t1\n",
      "  (135, 0)\t1\n",
      "  (139, 0)\t1\n",
      "  :\t:\n",
      "  (15952, 0)\t1\n",
      "  (15953, 0)\t1\n",
      "  (15954, 0)\t1\n",
      "  (15955, 0)\t1\n",
      "  (15956, 0)\t1\n",
      "  (15957, 0)\t1\n",
      "  (15958, 0)\t1\n",
      "  (15959, 0)\t1\n",
      "  (15960, 0)\t1\n",
      "  (15961, 0)\t1\n",
      "  (15962, 0)\t1\n",
      "  (15963, 0)\t1\n",
      "  (15964, 0)\t1\n",
      "  (15965, 0)\t1\n",
      "  (15966, 0)\t1\n",
      "  (15967, 0)\t1\n",
      "  (15968, 0)\t1\n",
      "  (15969, 0)\t1\n",
      "  (15970, 0)\t1\n",
      "  (15971, 0)\t1\n",
      "  (15972, 0)\t1\n",
      "  (15973, 0)\t1\n",
      "  (15974, 0)\t1\n",
      "  (15975, 0)\t1\n",
      "  (15976, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "# column names\n",
    "with open('data/column_names.txt', 'r') as f:\n",
    "    column_names = [line.strip() for line in f]\n",
    "\n",
    "sparse_dat = sparse.load_npz(\"data/sparse_df.npz\")\n",
    "\n",
    "# Extract labels from the first column\n",
    "labels = sparse_dat[:, 0]\n",
    "\n",
    "# Create a list of column indices to keep\n",
    "to_keep = list(set(range(sparse_dat.shape[1])) - set([0]))\n",
    "\n",
    "# Extract the design matrix\n",
    "X = sparse_dat[:, to_keep]\n",
    "\n",
    "print(f'labels: \\n{labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design matrix: \n",
      "  (17173, 0)\t1\n",
      "  (17240, 0)\t2\n",
      "  (17164, 1)\t1\n",
      "  (17363, 1)\t1\n",
      "  (17448, 1)\t1\n",
      "  (17910, 1)\t1\n",
      "  (17914, 1)\t1\n",
      "  (17933, 1)\t2\n",
      "  (15801, 2)\t1\n",
      "  (15867, 2)\t1\n",
      "  (16217, 2)\t1\n",
      "  (17173, 2)\t3\n",
      "  (17189, 2)\t1\n",
      "  (17386, 2)\t1\n",
      "  (17765, 2)\t4\n",
      "  (17933, 3)\t2\n",
      "  (17325, 4)\t1\n",
      "  (17366, 4)\t1\n",
      "  (16271, 5)\t1\n",
      "  (16837, 5)\t1\n",
      "  (17933, 6)\t2\n",
      "  (17933, 7)\t2\n",
      "  (17173, 8)\t2\n",
      "  (17933, 8)\t1\n",
      "  (15801, 9)\t1\n",
      "  :\t:\n",
      "  (8386, 56199)\t2\n",
      "  (8669, 56200)\t1\n",
      "  (10316, 56200)\t1\n",
      "  (8805, 56201)\t1\n",
      "  (9218, 56201)\t1\n",
      "  (9565, 56202)\t1\n",
      "  (9814, 56202)\t1\n",
      "  (6232, 56203)\t1\n",
      "  (9353, 56203)\t1\n",
      "  (5661, 56204)\t9\n",
      "  (6860, 56204)\t1\n",
      "  (8698, 56204)\t1\n",
      "  (125, 56205)\t1\n",
      "  (1228, 56205)\t1\n",
      "  (4422, 56205)\t1\n",
      "  (5648, 56206)\t1\n",
      "  (7464, 56206)\t1\n",
      "  (5648, 56207)\t2\n",
      "  (6025, 56208)\t2\n",
      "  (6025, 56209)\t2\n",
      "  (6151, 56210)\t2\n",
      "  (8669, 56211)\t1\n",
      "  (10316, 56211)\t1\n",
      "  (6340, 56212)\t1\n",
      "  (8696, 56212)\t1\n"
     ]
    }
   ],
   "source": [
    "print(f'Design matrix: \\n{X}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test split\n",
    "To do: consider stratifying by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14524, 56213)\n",
      "(14524, 1)\n",
      "(3632, 56213)\n",
      "(3632, 1)\n",
      "int64\n",
      "proportion of spam in training data: 0.39568989259157256\n",
      "proportion of spam in testing data: 0.11921806167400881\n"
     ]
    }
   ],
   "source": [
    "# To do - stratify the split \n",
    "n_samples = labels.shape[0]\n",
    "# Use train_test_split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.dtype)\n",
    "print(\"proportion of spam in training data:\", (y_train == 1).sum().item() / y_train.shape[0])\n",
    "print(\"proportion of spam in testing data:\", (y_test == 1).sum().item() / y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(y, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the proportion of type 2 errors - when the true label is 1 - spam, and the predicted label is 0 - ham\n",
    "\n",
    "        Args:\n",
    "        y: true labels\n",
    "        y_pred: predicted labels\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        type2errors = ((y == 1) & (y_pred == 0)).sum().item()\n",
    "        type1errors = ((y == 0) & (y_pred == 1)).sum().item()\n",
    "        correct = (y_pred == y).sum().item()\n",
    "        return type2errors, type1errors, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test models\n",
    "\n",
    "2 models:\n",
    "1. Without penalty\n",
    "2. With penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run AdaBoostWeak.py\n",
    "\n",
    "aboost1 = AdaBoostWeak(type2penalty = False, rounds = 200, maxDTdepth = 5)\n",
    "aboost1.fit(X = X_train, y = y_train.toarray().ravel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (no penalty) Training set Accuracy:  0.9839575874414762\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 122 \n",
      " type 1 errors: 111\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost1.predict(X_train)\n",
    "type2, type1, correct = errors(y_train.toarray().ravel(), predictions)\n",
    "print(\"Model 1 (no penalty) Training set Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Test Accuracy:  0.6126101321585903\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 13 \n",
      " type 1 errors: 1394\n",
      "[1.9311982474293699, 1.1922207924007637, 0.638792543336037, 0.9565302233775526, 0.8196529928910588, 0.697405027301431, 0.6268243712472432, 0.5559371692984132, 0.5609604375911083, 0.4737119147887212, 0.46470756312763584, 0.39633071323129737, 0.3848666495528406, 0.3520962531430082, 0.4276826263393198, 0.3610083791196701, 0.38468150252053895, 0.3379871316043892, 0.32271472937161083, 0.70611638673965, 0.37779707794666806, 0.4165416675433389, 0.4593787147307749, 0.36060203434342486, 0.2624554263235654, 0.3049512463605404, 0.321963451591486, 0.2854360711001694, 0.24097599418644053, 0.2655480312920202, 0.28898811726827794, 0.2925911905108335, 0.2594779945746731, 0.3324960083596197, 0.3949602326738274, 0.3098108196138844, 0.27714383510154733, 0.2678651886564627, 0.22559934462655523, 0.342863738697453, 0.30857697728995603, 0.265679517179477, 0.29804466274520586, 0.2518674969400665, 0.11071945336714288, 0.13207471378794852, 0.22685816849392043, 0.23246313327641438, 0.1346101728737038, 0.16120154483403237, 0.21737507092099706, 0.2652340585309419, 0.2521397851481615, 0.20389102339092827, 0.22199376727129555, 0.3304342424780293, 0.26422889275684613, 0.21607989408693504, 0.2240723430805719, 0.19881039242107945, 0.1779408542893972, 0.2712447829427386, 0.26559687426380046, 0.19116806544862314, 0.13540529038779506, 0.14592386327631202, 0.19378685298304002, 0.2532043456244615, 0.3057104319207337, 0.23643699426274334, 0.19440211000685234, 0.2220132117175953, 0.20451840912857472, 0.17661301295084655, 0.17673160324225917, 0.2151814208222179, 0.18558187914814064, 0.16267643162892506, 0.1331165554402734, 0.19164242209708574, 0.20696745634947675, 0.156558321670855, 0.12042703604284113, 0.12966334623896394, 0.3969361228604918, 0.3663543952896528, 0.40617762105270433, 0.4416886383728693, 0.37041193722551424, 0.2568037208755836, 0.11456691870474012, 0.2361808589891492, 0.19797948811346644, 0.15401785027022213, 0.1845867460074709, 0.11856747017372278, 0.10451892541113136, 0.1472796315129394, 0.1474914340571363, 0.11727423357433224, 0.13082236854909116, 0.2194671135373434, 0.21576885075486826, 0.1789950052269962, 0.20827378796472804, 0.20090936842202411, 0.2009479192228217, 0.18731386842591868, 0.19032675482885536, 0.2929738240896956, 0.20959283555001612, 0.22649337856348006, 0.2347917963916445, 0.1664933402030239, 0.14724834736277242, 0.15298921766502624, 0.11214863186147242, 0.11064582948678489, 0.1854281738892367, 0.17693451592635756, 0.15353318841324204, 0.1569238569264509, 0.13235606466278257, 0.14053074581813232, 0.130821540292865, 0.18040282984029615, 0.20506109374012974, 0.17148256549200758, 0.16882588021071612, 0.1559709070751488, 0.1211661276494576, 0.12941641889739566, 0.11982725456526724, 0.1801898170921972, 0.1751368280956101, 0.16948080566738172, 0.18148852958424327, 0.1696734485067855, 0.13357876423662915, 0.11124506226750949, 0.09836535856389399, 0.09143029231437451, 0.10922705942507337, 0.1334721237993362, 0.17038129413225844, 0.14720456989474587, 0.14731716012788712, 0.14766261401831535, 0.12330972067132617, 0.17249897821045276, 0.18166071060046202, 0.1448712497237986, 0.0981440390465705, 0.1568135787904216, 0.18226706351041785, 0.1579602375738787, 0.1580894671992136, 0.2232195833156513, 0.17564460181819014, 0.10577583052738386, 0.12841680999754615, 0.11118446389256094, 0.14972325722849572, 0.1981137048380715, 0.15783651588230982, 0.12047217249718288, 0.13896551490363096, 0.14762174138538484, 0.13835771668966376, 0.15260810895113502, 0.17868985014355812, 0.14778370519492112, 0.12558437408367906, 0.10311237077636437, 0.1044430603268314, 0.07686581306697989, 0.11094321135858944, 0.16785959915184692, 0.11405958579523859, 0.06503984379123869, 0.0902198235255499, 0.13582831461424788, 0.13037844284852132, 0.13388752978404536, 0.14879562745490127, 0.1742166456220626, 0.12335127380372071, 0.07452885512852185, 0.15537438346060978, 0.11552156183153303, 0.15203025075204976, 0.2879721548987708, 0.217629451058859, 0.12659100507900856, 0.11449234734714574, 0.11391312308390453, 0.08784152126996413, 0.14164537636135863, 0.1864267110414781, 0.13501394409549045]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost1.predict(X_test)\n",
    "type2, type1, correct = errors(y_test.toarray().ravel(), predictions)\n",
    "print(\"Model 1 Test Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')\n",
    "#print(aboost1.alphas)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run AdaBoostWeak.py\n",
    "\n",
    "aboost2 = AdaBoostWeak(type2penalty = True, rounds = 200, maxDTdepth = 5)\n",
    "aboost2.fit(X = X_train, y = y_train.toarray().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 (with penalty) Training set Accuracy:  0.9998622968879096\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 2 \n",
      " type 1 errors: 0\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost2.predict(X_train)\n",
    "type2, type1, correct = errors(y_train.toarray().ravel(), predictions)\n",
    "print(\"Model 2 (with penalty) Training set Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 Test Accuracy:  0.7213656387665198\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 26 \n",
      " type 1 errors: 986\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost2.predict(X_test)\n",
    "type2, type1, correct = errors(y_test.toarray().ravel(), predictions)\n",
    "print(\"Model 2 Test Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')\n",
    "# print(aboost1.alphas)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning/Hyperparameter search\n",
    "\n",
    "Sigh. So much overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross val scores  [0.82126136 0.95951529 0.90994216 0.86036904]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=9)\n",
    "cvs = cross_val_score(clf, X_train,predictions,cv=4)\n",
    "print(\"cross val scores \",cvs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #checkity check\n",
    "# from importlib import reload\n",
    "# import AdaBoostWeak\n",
    "# reload(AdaBoostWeak)\n",
    "# from AdaBoostWeak import AdaBoostWeak\n",
    "# a= AdaBoostWeak(rounds=100)\n",
    "# a.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #clonity checkity check\n",
    "# from sklearn.base import clone\n",
    "# ada = AdaBoostWeak(rounds=100, type2penalty=False, maxDTdepth=1)  # Replace with actual parameters\n",
    "# ada_clone = clone(ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(14524, 1)\n",
      "(14524,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m y_train_dense \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mtoarray()\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_train_dense\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 33\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m \u001b[43mGridSearchCV\u001b[49m(aboost, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     34\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_dense)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "# #not adaboost classifier woops\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# clf = DecisionTreeClassifier()\n",
    "# param_grid = [\n",
    "#     {'type2penalty': [False] ,'rounds': [200, 300, 400, 500], 'maxDTdepth': [5,6,7,8,9,10]},\n",
    "#     {'type2penalty': [True] ,'rounds': [200, 300, 400, 500], 'maxDTdepth': [5,6,7,8,9,10]}\n",
    "#     ]\n",
    "\n",
    "# print(type(y_train))\n",
    "# print(y_train.shape)\n",
    "# y_train_dense = y_train.toarray().ravel()\n",
    "# print(y_train.shape)\n",
    "\n",
    "# grid_search = GridSearchCV(clf, param_grid, cv=4)\n",
    "# grid_search.fit(X_train, y_train_dense)\n",
    "\n",
    "# print(grid_search.best_params_)\n",
    "\n",
    "\n",
    "aboost = AdaBoostWeak(rounds=100)\n",
    "param_grid = [\n",
    "    {'type2penalty': [False], 'rounds': [200, 300, 400, 500], 'maxDTdepth': [5, 6, 7, 8, 9, 10]},\n",
    "    {'type2penalty': [True], 'rounds': [200, 300, 400, 500], 'maxDTdepth': [5, 6, 7, 8, 9, 10]}\n",
    "]\n",
    "\n",
    "print(type(y_train))\n",
    "print(y_train.shape)\n",
    "\n",
    "y_train_dense = y_train.toarray().ravel()\n",
    "\n",
    "print(y_train_dense.shape)\n",
    "grid_search = GridSearchCV(aboost, param_grid, cv=4)\n",
    "grid_search.fit(X_train, y_train_dense)\n",
    "\n",
    "print(\"best params:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "param_grid = [\n",
    "    {'type2penalty': [False] ,'rounds': [200, 300, 400, 500], 'maxDTdepth': [5,6,7,8,9,10]},\n",
    "    {'type2penalty': [True] ,'rounds': [200, 300, 400, 500], 'maxDTdepth': [5,6,7,8,9,10]}\n",
    "    ]\n",
    "\n",
    "\n",
    "model = AdaBoostWeak(type2penalty=False, maxDTdepth=5,rounds=100)\n",
    "scorer = make_scorer(accuracy_score)\n",
    "grid_search=GridSearchCV(estimator=model, param_grid=param_grid, scoring=scorer, cv=5)\n",
    "grid_search.fit(X_train, y_train.toarray().ravel())\n",
    "\n",
    "print(\"best params:\", grid_search.best_params_)\n",
    "print(\"best cross val score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
