{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take 2: Adaboost with Weak Classifiers\n",
    "None of that bagging classifier bullshit\n",
    "\n",
    "jonat version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import guys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "# from AdaBoostClassifier import AdaBoostClassifier\n",
    "#%run AdaBoostWeak.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: \n",
      "  (2, 0)\t1\n",
      "  (5, 0)\t1\n",
      "  (8, 0)\t1\n",
      "  (9, 0)\t1\n",
      "  (11, 0)\t1\n",
      "  (12, 0)\t1\n",
      "  (15, 0)\t1\n",
      "  (19, 0)\t1\n",
      "  (34, 0)\t1\n",
      "  (42, 0)\t1\n",
      "  (54, 0)\t1\n",
      "  (56, 0)\t1\n",
      "  (65, 0)\t1\n",
      "  (67, 0)\t1\n",
      "  (68, 0)\t1\n",
      "  (93, 0)\t1\n",
      "  (95, 0)\t1\n",
      "  (114, 0)\t1\n",
      "  (117, 0)\t1\n",
      "  (120, 0)\t1\n",
      "  (121, 0)\t1\n",
      "  (123, 0)\t1\n",
      "  (134, 0)\t1\n",
      "  (135, 0)\t1\n",
      "  (139, 0)\t1\n",
      "  :\t:\n",
      "  (15952, 0)\t1\n",
      "  (15953, 0)\t1\n",
      "  (15954, 0)\t1\n",
      "  (15955, 0)\t1\n",
      "  (15956, 0)\t1\n",
      "  (15957, 0)\t1\n",
      "  (15958, 0)\t1\n",
      "  (15959, 0)\t1\n",
      "  (15960, 0)\t1\n",
      "  (15961, 0)\t1\n",
      "  (15962, 0)\t1\n",
      "  (15963, 0)\t1\n",
      "  (15964, 0)\t1\n",
      "  (15965, 0)\t1\n",
      "  (15966, 0)\t1\n",
      "  (15967, 0)\t1\n",
      "  (15968, 0)\t1\n",
      "  (15969, 0)\t1\n",
      "  (15970, 0)\t1\n",
      "  (15971, 0)\t1\n",
      "  (15972, 0)\t1\n",
      "  (15973, 0)\t1\n",
      "  (15974, 0)\t1\n",
      "  (15975, 0)\t1\n",
      "  (15976, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "# column names\n",
    "with open('data/column_names.txt', 'rb') as f:\n",
    "    column_names = [line.strip() for line in f]\n",
    "\n",
    "sparse_dat = sparse.load_npz(\"data/sparse_df.npz\")\n",
    "\n",
    "# Extract labels from the first column\n",
    "labels = sparse_dat[:, 0]\n",
    "\n",
    "# Create a list of column indices to keep\n",
    "to_keep = list(set(range(sparse_dat.shape[1])) - set([0]))\n",
    "\n",
    "# Extract the design matrix\n",
    "X = sparse_dat[:, to_keep]\n",
    "\n",
    "print(f'labels: \\n{labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design matrix: \n",
      "  (17173, 0)\t1\n",
      "  (17240, 0)\t2\n",
      "  (17164, 1)\t1\n",
      "  (17363, 1)\t1\n",
      "  (17448, 1)\t1\n",
      "  (17910, 1)\t1\n",
      "  (17914, 1)\t1\n",
      "  (17933, 1)\t2\n",
      "  (15801, 2)\t1\n",
      "  (15867, 2)\t1\n",
      "  (16217, 2)\t1\n",
      "  (17173, 2)\t3\n",
      "  (17189, 2)\t1\n",
      "  (17386, 2)\t1\n",
      "  (17765, 2)\t4\n",
      "  (17933, 3)\t2\n",
      "  (17325, 4)\t1\n",
      "  (17366, 4)\t1\n",
      "  (16271, 5)\t1\n",
      "  (16837, 5)\t1\n",
      "  (17933, 6)\t2\n",
      "  (17933, 7)\t2\n",
      "  (17173, 8)\t2\n",
      "  (17933, 8)\t1\n",
      "  (15801, 9)\t1\n",
      "  :\t:\n",
      "  (8386, 56199)\t2\n",
      "  (8669, 56200)\t1\n",
      "  (10316, 56200)\t1\n",
      "  (8805, 56201)\t1\n",
      "  (9218, 56201)\t1\n",
      "  (9565, 56202)\t1\n",
      "  (9814, 56202)\t1\n",
      "  (6232, 56203)\t1\n",
      "  (9353, 56203)\t1\n",
      "  (5661, 56204)\t9\n",
      "  (6860, 56204)\t1\n",
      "  (8698, 56204)\t1\n",
      "  (125, 56205)\t1\n",
      "  (1228, 56205)\t1\n",
      "  (4422, 56205)\t1\n",
      "  (5648, 56206)\t1\n",
      "  (7464, 56206)\t1\n",
      "  (5648, 56207)\t2\n",
      "  (6025, 56208)\t2\n",
      "  (6025, 56209)\t2\n",
      "  (6151, 56210)\t2\n",
      "  (8669, 56211)\t1\n",
      "  (10316, 56211)\t1\n",
      "  (6340, 56212)\t1\n",
      "  (8696, 56212)\t1\n"
     ]
    }
   ],
   "source": [
    "print(f'Design matrix: \\n{X}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test split\n",
    "To do: consider stratifying by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14524, 56213)\n",
      "(14524, 1)\n",
      "(3632, 56213)\n",
      "(3632, 1)\n",
      "int64\n",
      "proportion of spam in training data: 0.3404020930873038\n",
      "proportion of spam in testing data: 0.34030837004405284\n"
     ]
    }
   ],
   "source": [
    "# To do - stratify the split \n",
    "n_samples = labels.shape[0]\n",
    "# Use train_test_split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, shuffle=True, stratify=labels.toarray().ravel())\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.dtype)\n",
    "print(\"proportion of spam in training data:\", (y_train == 1).sum().item() / y_train.shape[0])\n",
    "print(\"proportion of spam in testing data:\", (y_test == 1).sum().item() / y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(y, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the proportion of type 2 errors - when the true label is 1 - spam, and the predicted label is 0 - ham\n",
    "\n",
    "        Args:\n",
    "        y: true labels\n",
    "        y_pred: predicted labels\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        y[y == -1] = 0\n",
    "        type2errors = ((y == 1) & (y_pred == 0)).sum().item()\n",
    "        type1errors = ((y == 0) & (y_pred == 1)).sum().item()\n",
    "        correct = (y_pred == y).sum().item()\n",
    "        #print(np.unique(y_pred), np.unique(y))\n",
    "        return type2errors, type1errors, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test models\n",
    "\n",
    "2 models:\n",
    "1. Without penalty\n",
    "2. With penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run AdaBoostWeakClassicRF.py\n",
    "\n",
    "aboost1 = AdaBoostWeakClassicRF(type2penalty = False, rounds = 200, maxDTdepth = 5)\n",
    "aboost1.fit(X = X_train, y = y_train.toarray().ravel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (no penalty) Training set Accuracy:  0.3941063068025337\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 0 \n",
      " type 1 errors: 8800\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost1.predict(X_train)\n",
    "type2, type1, correct = errors(y_train.toarray().ravel(), predictions)\n",
    "print(\"Model 1 (no penalty) Training set Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Test Accuracy:  0.39069383259911894\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 0 \n",
      " type 1 errors: 2213\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost1.predict(X_test)\n",
    "type2, type1, correct = errors(y_test.toarray().ravel(), predictions)\n",
    "print(\"Model 1 Test Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')\n",
    "# print(aboost1.alphas)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run AdaBoostWeakClassicRF.py\n",
    "\n",
    "aboost2 = AdaBoostWeakClassicRF(type2penalty = True, rounds = 200, maxDTdepth = 5, pen_factor = .99999)\n",
    "aboost2.fit(X = X_train, y = y_train.toarray().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 (with penalty) Training set Accuracy:  0.39486367391903054\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 0 \n",
      " type 1 errors: 8789\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost2.predict(X_train)\n",
    "type2, type1, correct = errors(y_train.toarray().ravel(), predictions)\n",
    "print(\"Model 2 (with penalty) Training set Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 Test Accuracy:  0.9028083700440529\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 322 \n",
      " type 1 errors: 31\n",
      "[0 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost2.predict(X_test)\n",
    "type2, type1, correct = errors(y_test.toarray().ravel(), predictions)\n",
    "print(\"Model 2 Test Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')\n",
    "# print(aboost1.alphas)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning/Hyperparameter search\n",
    "\n",
    "Sigh. So much overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = \\\n",
    "{'type2penalty': [False, True] ,'rounds': [200, 350 ,500], 'maxDTdepth': [5,10], 'pen_factor': [0.1, 0.3, 0.5, 0.7, 1.2, 1.5, 1.7, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train2 shape (12708, 56213) \n",
      "y_train2 shape (12708, 1) \n",
      "X_val shape (1816, 56213) \n",
      "y_val shape (1816, 1) \n"
     ]
    }
   ],
   "source": [
    "X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.125, shuffle=True, stratify= y_train.toarray().ravel())\n",
    "\n",
    "print(f'X_train2 shape {X_train2.shape} \\ny_train2 shape {y_train2.shape} \\nX_val shape {X_val.shape} \\ny_val shape {y_val.shape} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model without penalty | rounds: 200 | maxDTdepth: 5\n",
      "Training Accuracy:  0.39361032420522507\n",
      "Training correct count:  5002 len of predictions 12708\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 7706\n",
      "Validation Accuracy:  0.3904185022026432\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 1107\n",
      "\n",
      "Model with penalty: 0.1 | rounds: 200 | maxDTdepth: 5\n",
      "Training Accuracy:  0.34214667925716086\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 8360\n",
      "Validation Accuracy:  0.3436123348017621\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 1192\n",
      "\n",
      "Model with penalty: 0.3 | rounds: 200 | maxDTdepth: 5\n",
      "Training Accuracy:  0.34191060749134405\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 8363\n",
      "Validation Accuracy:  0.3419603524229075\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 1195\n",
      "\n",
      "Model with penalty: 0.5 | rounds: 200 | maxDTdepth: 5\n",
      "Training Accuracy:  0.34230406043437206\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 8358\n",
      "Validation Accuracy:  0.3419603524229075\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 1195\n",
      "\n",
      "Model with penalty: 0.7 | rounds: 200 | maxDTdepth: 5\n",
      "Training Accuracy:  0.34222536984576646\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 8359\n",
      "Validation Accuracy:  0.3419603524229075\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 1195\n",
      "\n",
      "Model with penalty: 1.2 | rounds: 200 | maxDTdepth: 5\n",
      "Training Accuracy:  0.9459395656279509\n",
      "type 2 errors: 348 \n",
      "type 1 errors: 339\n",
      "Validation Accuracy:  0.9443832599118943\n",
      "type 2 errors: 46 \n",
      "type 1 errors: 55\n",
      "\n",
      "Model with penalty: 1.5 | rounds: 200 | maxDTdepth: 5\n",
      "Training Accuracy:  0.9263456090651558\n",
      "type 2 errors: 261 \n",
      "type 1 errors: 675\n",
      "Validation Accuracy:  0.9284140969162996\n",
      "type 2 errors: 32 \n",
      "type 1 errors: 98\n",
      "\n",
      "Model with penalty: 1.7 | rounds: 200 | maxDTdepth: 5\n",
      "Training Accuracy:  0.8073654390934845\n",
      "type 2 errors: 2417 \n",
      "type 1 errors: 31\n",
      "Validation Accuracy:  0.8072687224669604\n",
      "type 2 errors: 348 \n",
      "type 1 errors: 2\n",
      "\n",
      "Model with penalty: 2 | rounds: 200 | maxDTdepth: 5\n",
      "Training Accuracy:  0.9201290525653132\n",
      "type 2 errors: 612 \n",
      "type 1 errors: 403\n",
      "Validation Accuracy:  0.9162995594713657\n",
      "type 2 errors: 90 \n",
      "type 1 errors: 62\n",
      "\n",
      "Model without penalty | rounds: 200 | maxDTdepth: 10\n",
      "Training Accuracy:  0.47820270695624806\n",
      "Training correct count:  6077 len of predictions 12708\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 6631\n",
      "Validation Accuracy:  0.4906387665198238\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 925\n",
      "\n",
      "Model with penalty: 0.1 | rounds: 200 | maxDTdepth: 10\n",
      "Training Accuracy:  0.3641800440667296\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 8080\n",
      "Validation Accuracy:  0.3601321585903084\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 1162\n",
      "\n",
      "Model with penalty: 0.3 | rounds: 200 | maxDTdepth: 10\n",
      "Training Accuracy:  0.3719704123386843\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 7981\n",
      "Validation Accuracy:  0.3661894273127753\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 1151\n",
      "\n",
      "Model with penalty: 0.5 | rounds: 200 | maxDTdepth: 10\n",
      "Training Accuracy:  0.3611898016997167\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 8118\n",
      "Validation Accuracy:  0.3579295154185022\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 1166\n",
      "\n",
      "Model with penalty: 0.7 | rounds: 200 | maxDTdepth: 10\n",
      "Training Accuracy:  0.364494806421152\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 8076\n",
      "Validation Accuracy:  0.3606828193832599\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 1161\n",
      "\n",
      "Model with penalty: 1.2 | rounds: 200 | maxDTdepth: 10\n",
      "Training Accuracy:  0.9988983317595216\n",
      "type 2 errors: 7 \n",
      "type 1 errors: 7\n",
      "Validation Accuracy:  0.9620044052863436\n",
      "type 2 errors: 29 \n",
      "type 1 errors: 40\n",
      "\n",
      "Model with penalty: 1.5 | rounds: 200 | maxDTdepth: 10\n",
      "Training Accuracy:  0.950267548001259\n",
      "type 2 errors: 537 \n",
      "type 1 errors: 95\n",
      "Validation Accuracy:  0.9262114537444934\n",
      "type 2 errors: 97 \n",
      "type 1 errors: 37\n",
      "\n",
      "Model with penalty: 1.7 | rounds: 200 | maxDTdepth: 10\n",
      "Training Accuracy:  0.9664778092540132\n",
      "type 2 errors: 85 \n",
      "type 1 errors: 341\n",
      "Validation Accuracy:  0.9438325991189427\n",
      "type 2 errors: 33 \n",
      "type 1 errors: 69\n",
      "\n",
      "Model with penalty: 2 | rounds: 200 | maxDTdepth: 10\n",
      "Training Accuracy:  0.9772584198929808\n",
      "type 2 errors: 101 \n",
      "type 1 errors: 188\n",
      "Validation Accuracy:  0.9553964757709251\n",
      "type 2 errors: 34 \n",
      "type 1 errors: 47\n",
      "\n",
      "Model without penalty | rounds: 350 | maxDTdepth: 5\n",
      "Training Accuracy:  0.396994019515266\n",
      "Training correct count:  5045 len of predictions 12708\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 7663\n",
      "Validation Accuracy:  0.3948237885462555\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 1099\n",
      "\n",
      "Model with penalty: 0.1 | rounds: 350 | maxDTdepth: 5\n",
      "Training Accuracy:  0.34246144161158326\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 8356\n",
      "Validation Accuracy:  0.34306167400881055\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 1193\n",
      "\n",
      "Model with penalty: 0.3 | rounds: 350 | maxDTdepth: 5\n",
      "Training Accuracy:  0.34230406043437206\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 8358\n",
      "Validation Accuracy:  0.34306167400881055\n",
      "type 2 errors: 0 \n",
      "type 1 errors: 1193\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nickgeissler/ML Group Project/ML_duh_guyz/adaboostweakclassifier_rf.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nickgeissler/ML%20Group%20Project/ML_duh_guyz/adaboostweakclassifier_rf.ipynb#X24sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m aboost\u001b[39m.\u001b[39mfit(X \u001b[39m=\u001b[39m X_train2, y \u001b[39m=\u001b[39m dense_y_train2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nickgeissler/ML%20Group%20Project/ML_duh_guyz/adaboostweakclassifier_rf.ipynb#X24sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# training reports\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nickgeissler/ML%20Group%20Project/ML_duh_guyz/adaboostweakclassifier_rf.ipynb#X24sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m predictions \u001b[39m=\u001b[39m aboost\u001b[39m.\u001b[39mpredict(X_train2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nickgeissler/ML%20Group%20Project/ML_duh_guyz/adaboostweakclassifier_rf.ipynb#X24sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m type2, type1, correct \u001b[39m=\u001b[39m errors(dense_y_train2, predictions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nickgeissler/ML%20Group%20Project/ML_duh_guyz/adaboostweakclassifier_rf.ipynb#X24sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel with penalty: \u001b[39m\u001b[39m{\u001b[39;00mpen_factor\u001b[39m}\u001b[39;00m\u001b[39m | rounds: \u001b[39m\u001b[39m{\u001b[39;00mrounds\u001b[39m}\u001b[39;00m\u001b[39m | maxDTdepth: \u001b[39m\u001b[39m{\u001b[39;00mmaxDTdepth\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ML Group Project/ML_duh_guyz/AdaBoostWeakClassicRF.py:137\u001b[0m, in \u001b[0;36mAdaBoostWeakClassicRF.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Predict class label for each weak classifier, weighted by alpha_m\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrounds):\n\u001b[0;32m--> 137\u001b[0m     y_pred_m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstumps[m]\u001b[39m.\u001b[39mpredict(X) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphas[m]\n\u001b[1;32m    138\u001b[0m     final_predictions \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m y_pred_m\n\u001b[1;32m    140\u001b[0m \u001b[39m# Estimate final predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:905\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    885\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[39m    Predict class for X.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m        The predicted classes.\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_proba(X)\n\u001b[1;32m    907\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    908\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39margmax(proba, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:958\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    953\u001b[0m all_proba \u001b[39m=\u001b[39m [\n\u001b[1;32m    954\u001b[0m     np\u001b[39m.\u001b[39mzeros((X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], j), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m    955\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39matleast_1d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_)\n\u001b[1;32m    956\u001b[0m ]\n\u001b[1;32m    957\u001b[0m lock \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mLock()\n\u001b[0;32m--> 958\u001b[0m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose, require\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msharedmem\u001b[39m\u001b[39m\"\u001b[39m)(\n\u001b[1;32m    959\u001b[0m     delayed(_accumulate_prediction)(e\u001b[39m.\u001b[39mpredict_proba, X, all_proba, lock)\n\u001b[1;32m    960\u001b[0m     \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\n\u001b[1;32m    961\u001b[0m )\n\u001b[1;32m    963\u001b[0m \u001b[39mfor\u001b[39;00m proba \u001b[39min\u001b[39;00m all_proba:\n\u001b[1;32m    964\u001b[0m     proba \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mapply_async(batch, callback\u001b[39m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:731\u001b[0m, in \u001b[0;36m_accumulate_prediction\u001b[0;34m(predict, X, out, lock)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_accumulate_prediction\u001b[39m(predict, X, out, lock):\n\u001b[1;32m    725\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m    This is a utility function for joblib's Parallel.\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \n\u001b[1;32m    728\u001b[0m \u001b[39m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[39m    complains that it cannot pickle it when placed there.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 731\u001b[0m     prediction \u001b[39m=\u001b[39m predict(X, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    732\u001b[0m     \u001b[39mwith\u001b[39;00m lock:\n\u001b[1;32m    733\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/tree/_classes.py:1043\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.predict_proba\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m   1041\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m   1042\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_X_predict(X, check_input)\n\u001b[0;32m-> 1043\u001b[0m proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree_\u001b[39m.\u001b[39mpredict(X)\n\u001b[1;32m   1045\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1046\u001b[0m     \u001b[39mreturn\u001b[39;00m proba[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_]\n",
      "File \u001b[0;32msklearn/tree/_tree.pyx:970\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/tree/_tree.pyx:975\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/sparse/_base.py:119\u001b[0m, in \u001b[0;36mspmatrix.get_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m     new_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreshape(shape, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39masformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat)\n\u001b[1;32m    117\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39m=\u001b[39m new_matrix\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[0;32m--> 119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_shape\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    120\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get shape of a matrix.\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run AdaBoostWeakClassicRF.py\n",
    "for rounds in param_grid['rounds']:\n",
    "    for maxDTdepth in param_grid['maxDTdepth']:\n",
    "        dense_y_train2 = y_train2.toarray().ravel()\n",
    "        aboost = AdaBoostWeakClassicRF(type2penalty = False, rounds = rounds, maxDTdepth = maxDTdepth)\n",
    "        aboost.fit(X = X_train2, y = dense_y_train2)\n",
    "\n",
    "        # training reports\n",
    "        predictions = aboost.predict(X_train2)\n",
    "        type2, type1, correct = errors(dense_y_train2, predictions)\n",
    "        #print(f'prediction shape {predictions.shape}, true labels shape {dense_y_train2.shape}')\n",
    "        print(f\"Model without penalty | rounds: {rounds} | maxDTdepth: {maxDTdepth}\")\n",
    "        print(\"Training Accuracy: \", correct/len(predictions))\n",
    "        print(\"Training correct count: \", correct, \"len of predictions\", len(predictions))\n",
    "        print(f'type 2 errors: {type2} \\ntype 1 errors: {type1}')\n",
    "\n",
    "        # validation reports\n",
    "        dense_y_val = y_val.toarray().ravel()\n",
    "        predictions = aboost.predict(X_val)\n",
    "        type2, type1, correct = errors(dense_y_val, predictions)\n",
    "        print(\"Validation Accuracy: \", correct/len(predictions))\n",
    "        print(f'type 2 errors: {type2} \\ntype 1 errors: {type1}\\n')\n",
    "        \n",
    "        for pen_factor in param_grid['pen_factor']:\n",
    "            aboost = AdaBoostWeakClassicRF(type2penalty = True, rounds = rounds, maxDTdepth = maxDTdepth, pen_factor=pen_factor)\n",
    "            aboost.fit(X = X_train2, y = dense_y_train2)\n",
    "\n",
    "            # training reports\n",
    "            predictions = aboost.predict(X_train2)\n",
    "            type2, type1, correct = errors(dense_y_train2, predictions)\n",
    "            print(f\"Model with penalty: {pen_factor} | rounds: {rounds} | maxDTdepth: {maxDTdepth}\")\n",
    "            print(\"Training Accuracy: \", correct/len(predictions))\n",
    "            print(f'type 2 errors: {type2} \\ntype 1 errors: {type1}')\n",
    "\n",
    "            # testing reports\n",
    "            predictions = aboost.predict(X_val)\n",
    "            type2, type1, correct = errors(dense_y_val, predictions)\n",
    "            print(\"Validation Accuracy: \", correct/len(predictions))\n",
    "            print(f'type 2 errors: {type2} \\ntype 1 errors: {type1}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "\n",
    "F1 score, confusion matrix, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
