{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import guys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we weight our weak learner on the proportion of type 2 errors to n(number of samples) instead\n",
    "Say the proportion of type 2 errors is p. We weight our weak learner by 1-p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "  \"\"\"\n",
    "  Decision tree node class.\n",
    "  \"\"\"\n",
    "  def __init__(self, feature_index=None, threshold=None, left_child=None, right_child=None, class_label=None):\n",
    "    self.feature_index = feature_index  # Index of the feature used for splitting\n",
    "    self.threshold = threshold  # Threshold value for splitting\n",
    "    self.left_child = left_child  # Left child node\n",
    "    self.right_child = right_child  # Right child node\n",
    "    self.class_label = class_label  # Class label (for leaf nodes)\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "  \"\"\"\n",
    "  Simple Decision Tree Classifier.\n",
    "  \"\"\"\n",
    "  def __init__(self, max_depth=2):\n",
    "    self.max_depth = max_depth\n",
    "    self.root = None\n",
    "\n",
    "  def _entropy(self, y):\n",
    "    \"\"\"\n",
    "    Calculate entropy of a label distribution.\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    p = counts / len(y)\n",
    "    return -np.sum(p * np.log2(p + 1e-10))\n",
    "\n",
    "  def _information_gain(self, parent_entropy, left_entropy, right_entropy, weights):\n",
    "    \"\"\"\n",
    "    Calculate information gain for a split.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "      weights = np.ones(len(parent_entropy))\n",
    "    return parent_entropy - (np.sum(weights[left_entropy.index] * left_entropy) +\n",
    "                             np.sum(weights[right_entropy.index] * right_entropy))\n",
    "\n",
    "  def _find_best_split(self, X, y, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Find the best feature and threshold for splitting.\n",
    "    \"\"\"\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    best_gain = 0\n",
    "    n_features = X.shape[1]\n",
    "    parent_entropy = self._entropy(y)\n",
    "\n",
    "    for feature_index in range(n_features):\n",
    "      unique_values = np.unique(X[:, feature_index])\n",
    "      for threshold in (unique_values[:-1] + unique_values[1:]) / 2:\n",
    "        left_idx = X[:, feature_index] <= threshold\n",
    "        right_idx = ~left_idx\n",
    "        if sample_weight is None:\n",
    "          left_y, right_y = y[left_idx], y[right_idx]\n",
    "        else:\n",
    "          left_y, right_y = y[left_idx], y[right_idx]\n",
    "          left_weight, right_weight = sample_weight[left_idx], sample_weight[right_idx]\n",
    "        if len(left_y) == 0 or len(right_y) == 0:\n",
    "          continue\n",
    "\n",
    "        left_entropy = self._entropy(left_y)\n",
    "        right_entropy = self._entropy(right_y)\n",
    "        gain = self._information_gain(parent_entropy, left_entropy, right_entropy, left_weight if sample_weight is not None else None)\n",
    "        if gain > best_gain:\n",
    "          best_feature = feature_index\n",
    "          best_threshold = threshold\n",
    "          best_gain = gain\n",
    "\n",
    "    return best_feature, best_threshold\n",
    "\n",
    "  def _build_tree(self, X, y, sample_weight=None, depth=0):\n",
    "    \"\"\"\n",
    "    Recursively build the decision tree.\n",
    "    \"\"\"\n",
    "    if depth >= self.max_depth or len(np.unique(y)) == 1:\n",
    "        return DecisionTreeNode(class_label=np.argmax(np.average(y, weights=sample_weight)))\n",
    "\n",
    "    best_feature, best_threshold = self._find_best_split(X, y, sample_weight)\n",
    "    left_idx = X[:, best_feature] <= best_threshold\n",
    "    right_idx = ~left_idx\n",
    "    left_child = self._build_tree(X[left_idx], y[left_idx], sample_weight[left_idx] if sample_weight is not None else None, depth + 1)\n",
    "    right_child = self._build_tree(X[right_idx], y[right_idx], sample_weight[right_idx] if sample_weight is not None else None, depth + 1)\n",
    "\n",
    "    # Create and return the current node (root or internal node)\n",
    "    return DecisionTreeNode(feature_index=best_feature, \n",
    "                            threshold=best_threshold,\n",
    "                             left_child=left_child, \n",
    "                             right_child=right_child) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class\n",
    "\n",
    "class AdaBoostClassifer:\n",
    "    \"\"\"\n",
    "    Implementation of adaboost - with custom loss function\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators, lr, type2penalty = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_estimators = number of stumps the final tree will be built from\n",
    "            lr = learning rate\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.lr = lr\n",
    "        self.stumps = []\n",
    "        self.weights = None\n",
    "        self.penalty = type2penalty\n",
    "\n",
    "    def exp_loss(self, error):\n",
    "        # -1 is ham, 1 is spam\n",
    "        #loss = np.exp(-1 * y_true * y_pred)\n",
    "        loss = np.exp(error)\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the AdaBoost classifier.\n",
    "\n",
    "        Args:\n",
    "        X: Training data (2D array).\n",
    "        y: Training labels (1D array).\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.ones(n_samples) / n_samples\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Fit a stump/weak learner\n",
    "            weak_learner = DecisionTreeClassifier()\n",
    "            weak_learner.fit(X, y, sample_weight=self.weights)\n",
    "\n",
    "            # Predict using the weak learner\n",
    "            y_pred = weak_learner.predict(X)\n",
    "\n",
    "            # Calculate the total error/loss of the weak learner - using 0/1 loss - can change this guy later\n",
    "            error = np.sum(self.weights * (y != y_pred))\n",
    "\n",
    "            # Check for termination (all weights are 0)\n",
    "            if error == 0.0:\n",
    "                break\n",
    "\n",
    "            # Update weights based on error\n",
    "            self.weights[y != y_pred] *= np.exp(self.learning_rate * error)\n",
    "            self.weights /= np.sum(self.weights)\n",
    "\n",
    "            # Store the weak learner and its weight (alpha)\n",
    "            alpha = self.learning_rate * np.log(1.0 / (error + 1e-10))\n",
    "            if self.type2penalty:\n",
    "                break\n",
    "            self.weak_learners.append((weak_learner, alpha))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for new data.\n",
    "\n",
    "        Args:\n",
    "        X: New data (2D array).\n",
    "\n",
    "        Returns:\n",
    "        Predicted class labels (1D array).\n",
    "        \"\"\"\n",
    "        predictions = np.zeros((X.shape[0], len(self.weak_learners)))\n",
    "        for i, (weak_learner, alpha) in enumerate(self.weak_learners):\n",
    "            predictions[:, i] = weak_learner.predict(X)\n",
    "\n",
    "        return np.sign(np.sum(alpha * predictions, axis=1))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
