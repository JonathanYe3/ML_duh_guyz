{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39GGICFzwDrj"
      },
      "source": [
        "# **Instructions**\n",
        "\n",
        "This document is a template, and you are not required to follow it exactly. However, the kinds of questions we ask here are the kinds of questions we want you to focus on. While you might have answered similar questions to these in your project presentations, we want you to go into a lot more detail in this write-up; you can refer to the Lab homeworks for ideas on how to present your data or results.\n",
        "\n",
        "You don't have to answer every question in this template, but you should answer roughly this many questions. Your answers to such questions should be paragraph-length, not just a bullet point. You likely still have questions of your own -- that's okay! We want you to convey what you've learned, how you've learned it, and demonstrate that the content from the course has influenced how you've thought about this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9VDfC-ie19P"
      },
      "source": [
        "# Project Name\n",
        "Project mentor: the GOAT Edward Wang\n",
        "\n",
        "Nick Geissler <ngeissl2@jh.edu>, Annie Wang <awang105@jh.edu>, Jonathan Ye <jye41@jh.edu>, Evan Zhu <ezhu13@jh.edu>\n",
        "\n",
        "Link_to_git_repo - edward says making it public is sus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqwI3PT-hBJo"
      },
      "source": [
        "# Outline and Deliverables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Af6y48e7HI"
      },
      "source": [
        "List the deliverables from your project proposal. For each uncompleted deliverable, please include a sentence or two on why you weren't able to complete it (e.g. \"decided to use an existing implementation instead\" or \"ran out of time\"). For each completed deliverable, indicate which section of this notebook covers what you did.\n",
        "\n",
        "If you spent substantial time on any aspects that weren't deliverables in your proposal, please list those under \"Additional Work\" and indicate where in the notebook you discuss them.\n",
        "\n",
        "### Uncompleted Deliverables\n",
        "1. \"Expect to complete #2\": we decided to use an existing implementation for our SVM\n",
        "2. ...\n",
        "\n",
        "\n",
        "### Completed Deliverables\n",
        "1. \"Must complete #1\": We discuss our dataset pre-processing [in \"Dataset\" below](#scrollTo=zFq-_D0khnhh&line=10&uniqifier=1).\n",
        "2. \"Must complete #2\": We discuss training our logistic regression baseline [in \"Baselines\" below](#scrollTo=oMyqHUa0jUw7&line=5&uniqifier=1).\n",
        "3. ...\n",
        "\n",
        "\n",
        "### Additional Deliverables\n",
        "1. We decided to add a second baseline using the published model from this paper. We discuss this [in \"Baselines\" below](#scrollTo=oMyqHUa0jUw7&line=5&uniqifier=1).\n",
        "2. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eiq2aSauhSsS"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtWkhiIPfOfK"
      },
      "source": [
        "## What problem were you trying to solve or understand?\n",
        "\n",
        "Q. What are the real-world implications of this data and task?\n",
        "\n",
        "A. \n",
        "\n",
        "Q. How is this problem similar to others weâ€™ve seen in lectures, breakouts, and homeworks?\n",
        "\n",
        "A. \n",
        "\n",
        "Q. What makes this problem unique?\n",
        "\n",
        "A. \n",
        "\n",
        "Q. What ethical implications does this problem have?\n",
        "\n",
        "A. What if the scammers get laid off from their cushy scamming cetner jobs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFq-_D0khnhh"
      },
      "source": [
        "## Dataset(s)\n",
        "\n",
        "Describe the dataset(s) you used.\n",
        "\n",
        "How were they collected?\n",
        "\n",
        "Why did you choose them?\n",
        "\n",
        "How many examples in each?\n",
        "* In total, we had ___ labeled emails and ___ features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import guys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "# from AdaBoostClassifier import AdaBoostClassifier\n",
        "#%run AdaBoostWeak.py\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2lOicoBYif7g"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Category                                            Message\n",
            "0      ham  Go until jurong point, crazy.. Available only ...\n",
            "1      ham                      Ok lar... Joking wif u oni...\n",
            "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n"
          ]
        }
      ],
      "source": [
        "# Load your data and print 2-3 examples\n",
        "df1 = pd.read_csv(\"data/spam.csv\", usecols=[0,1])\n",
        "print(df1.iloc[0:3,:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN1fYEfGidiD"
      },
      "source": [
        "## Pre-processing\n",
        "\n",
        "Please see our preprocessing2 jupyter notebook: {insert link}. \n",
        "\n",
        "### Questions\n",
        "\n",
        "What features did you use or choose not to use? Why?\n",
        "\n",
        "If you have categorical labels, were your datasets class-balanced?\n",
        "\n",
        "How did you deal with missing data? What about outliers?\n",
        "\n",
        "What approach(es) did you use to pre-process your data? Why?\n",
        "\n",
        "Are your features continuous or categorical? How do you treat these features differently?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEuKEzM5ipag"
      },
      "outputs": [],
      "source": [
        "# For those same examples above, what do they look like after being pre-processed?\n",
        "with open('data/column_names.txt', 'r') as f:\n",
        "    column_names = [line.strip() for line in f] # feature labels\n",
        "\n",
        "sparse_dat = sparse.load_npz(\"data/sparse_df.npz\")\n",
        "labels = sparse_dat[:, 0] # ground truth labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cDLEwhAx0gP"
      },
      "outputs": [],
      "source": [
        "# Visualize the distribution of your data before and after pre-processing.\n",
        "#   You may borrow from how we visualized data in the Lab homeworks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tASjmmtjiwvu"
      },
      "source": [
        "# Models and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlrwR9E1hnQ3"
      },
      "source": [
        "## Experimental Setup\n",
        "\n",
        "### Evaluation metrics\n",
        "\n",
        "Q. How did you evaluate your methods? Why is that a reasonable evaluation metric for the task?\n",
        "\n",
        "A. F1, type 1 error, type 2 error, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom loss function\n",
        "\n",
        "Q. What did you use for your loss function to train your models? Did you try multiple loss functions? Why or why not?\n",
        "\n",
        "A. Our loss function is pasted below. -INSERT DETAILS- We did try a couple other loss functions, but they sucked. There was that one loss function where it was np.exp(-proportion or whatever)\n",
        "\n",
        "Furthermore, we implemented our own Adaboost algorithm to leverage this custom loss function. Link:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUNxC358jPDr"
      },
      "outputs": [],
      "source": [
        "# Code for loss functions, evaluation metrics or link to Git repo\n",
        "\n",
        "def compute_error(y, y_pred, w_i, type2penalty, pen_factor):\n",
        "    '''\n",
        "    Calculate the error rate of a weak classifier m. Arguments:\n",
        "    y: actual target value\n",
        "    y_pred: predicted value by weak classifier\n",
        "    w_i: individual weights for each observation\n",
        "\n",
        "    \n",
        "    Note that all arrays should be the same length. Convert sparse array to regular array before calling\n",
        "    '''\n",
        "    if type2penalty:\n",
        "        error = (sum(w_i * (t2_pred_err_vec(y, y_pred, pen_factor)).astype(float)))/sum(w_i)\n",
        "    else:\n",
        "        error = (sum(w_i * (np.not_equal(y, y_pred)).astype(int)))/sum(w_i)\n",
        "\n",
        "    return error\n",
        "def t2_pred_err_vec(y,y_pred, pen_factor):\n",
        "    pred_err_vec = ((y_pred==-1) & (y==1))* pen_factor\n",
        "    better_err_vec = ((y_pred==1)) & (y==-1)\n",
        "    pred_err_vec = pred_err_vec+better_err_vec\n",
        "    if np.isnan(pred_err_vec).any(): print(\"WAHHHHH NAN\")\n",
        "    return pred_err_vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Test split\n",
        "\n",
        "Q. How did you split your data into train and test sets? Why?\n",
        "\n",
        "A. By the y labels - so that we had similar proportion of spam in both train + test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train test split\n",
        "\n",
        "# Use train_test_split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, shuffle=True, stratify=labels.toarray().ravel())\n",
        "\n",
        "y_train_flat = y_train.copy().toarray().ravel()\n",
        "#y_train_flat[y_train_flat == -1] = 0 See if you have to run this line if anything sus\n",
        "\n",
        "y_test_flat = y_test.copy().toarray().ravel()\n",
        "# y_test_flat[y_test_flat == -1] = 0 Same here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMyqHUa0jUw7"
      },
      "source": [
        "## Baselines\n",
        "\n",
        "Q. What baselines did you compare against? Why are these reasonable?\n",
        "\n",
        "A. Sklearn SVM and Random Forest with default parameters. WHY ARE REASONABLE\n",
        "\n",
        "Q. Did you look at related work to contextualize how others methods or baselines have performed on this dataset/task? If so, how did those methods do?\n",
        "\n",
        "A. Yes of course - they are very good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SVM\n",
        "\n",
        "from sklearn import svm\n",
        "svm_example = svm.SVC(probability = True)\n",
        "svm_example.fit(X_train, y_train_flat)\n",
        "\n",
        "# For ROC\n",
        "scores = svm_example.predict_proba(X_test)\n",
        "svm_fpr, svm_tpr, thresholds  = metrics.roc_curve(y_test_flat, scores[:,1])\n",
        "\n",
        "# Also do the type 2 error shit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train_flat)\n",
        "\n",
        "# For ROC\n",
        "scores = rf.predict_proba(X_test)\n",
        "rf_fpr, rf_tpr, thresholds = metrics.roc_curve(y_test_flat, scores[:,1])\n",
        "\n",
        "# type 2 error stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqB48IF9kMBf"
      },
      "source": [
        "## Methods\n",
        "\n",
        "Q. What methods did you choose? Why did you choose them?\n",
        "\n",
        "A. \n",
        "\n",
        "Q. How did you train these methods, and how did you evaluate them? Why?\n",
        "\n",
        "A. \n",
        "\n",
        "Q. Which methods were easy/difficult to implement and train? Why?\n",
        "\n",
        "A. Implementatino was decently hard. Big obstacle was getting the weak learners to be unique. Maybe also talk about boosting RF? Link github\n",
        "\n",
        "\n",
        "Q. For each method, what hyperparameters did you evaluate? How sensitive was your model's performance to different hyperparameter settings?\n",
        "\n",
        "A. First, we ran a grid search to find the best set of parameters for our goal - to reduce type 2 error (actual spam email classified as ham) while not sacrificing too much accuracy. Details are in the grid search section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grid Search\n",
        "\n",
        "EVAN WAHHHH - add details (explain validation set, cross fold, yada yada) and most importantly paste that figure in here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# do something here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model\n",
        "\n",
        "Using our choise of parameters from grid search:\n",
        "\n",
        "penalty factor = 2\n",
        "\n",
        "decision tree depth = 5 - for the sake of speed\n",
        "\n",
        "number of boosting rounds = 200 - for the sake of speed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma4JoDzar6xU"
      },
      "outputs": [],
      "source": [
        "# Code for training models, or link to your Git repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO_kP1fmkWWk"
      },
      "outputs": [],
      "source": [
        "# Show plots of how these models performed during training.\n",
        "#  For example, plot train loss and train accuracy (or other evaluation metric) on the y-axis,\n",
        "#  with number of iterations or number of examples on the x-axis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zdp4_H-kx8H"
      },
      "source": [
        "## Results\n",
        "\n",
        "Show tables comparing your methods to the baselines.\n",
        "\n",
        "What about these results surprised you? Why?\n",
        "\n",
        "Did your models over- or under-fit? How can you tell? What did you do to address these issues?\n",
        "\n",
        "What does the evaluation of your trained models tell you about your data? How do you expect these models might behave differently on different data?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS2sjfbglG_V"
      },
      "outputs": [],
      "source": [
        "# Show plots or visualizations of your evaluation metric(s) on the train and test sets.\n",
        "#   What do these plots show about over- or under-fitting?\n",
        "#   You may borrow from how we visualized results in the Lab homeworks.\n",
        "#   Are there aspects of your results that are difficult to visualize? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59EbS1GilSQ_"
      },
      "source": [
        "# Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugJXhZKNlUT4"
      },
      "source": [
        "## What you've learned\n",
        "\n",
        "*Note: you don't have to answer all of these, and you can answer other questions if you'd like. We just want you to demonstrate what you've learned from the project.*\n",
        "\n",
        "What concepts from lecture/breakout were most relevant to your project? How so?\n",
        "\n",
        "What aspects of your project did you find most surprising?\n",
        "\n",
        "What lessons did you take from this project that you want to remember for the next ML project you work on? Do you think those lessons would transfer to other datasets and/or models? Why or why not?\n",
        "\n",
        "What was the most helpful feedback you received during your presentation? Why?\n",
        "\n",
        "If you had two more weeks to work on this project, what would you do next? Why?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
