{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take 2: Adaboost with Weak Classifiers\n",
    "None of that bagging classifier bullshit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import guys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "# from AdaBoostClassifier import AdaBoostClassifier\n",
    "#%run AdaBoostWeak.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: \n",
      "  (2, 0)\t1\n",
      "  (5, 0)\t1\n",
      "  (8, 0)\t1\n",
      "  (9, 0)\t1\n",
      "  (11, 0)\t1\n",
      "  (12, 0)\t1\n",
      "  (15, 0)\t1\n",
      "  (19, 0)\t1\n",
      "  (34, 0)\t1\n",
      "  (42, 0)\t1\n",
      "  (54, 0)\t1\n",
      "  (56, 0)\t1\n",
      "  (65, 0)\t1\n",
      "  (67, 0)\t1\n",
      "  (68, 0)\t1\n",
      "  (93, 0)\t1\n",
      "  (95, 0)\t1\n",
      "  (114, 0)\t1\n",
      "  (117, 0)\t1\n",
      "  (120, 0)\t1\n",
      "  (121, 0)\t1\n",
      "  (123, 0)\t1\n",
      "  (134, 0)\t1\n",
      "  (135, 0)\t1\n",
      "  (139, 0)\t1\n",
      "  :\t:\n",
      "  (15952, 0)\t1\n",
      "  (15953, 0)\t1\n",
      "  (15954, 0)\t1\n",
      "  (15955, 0)\t1\n",
      "  (15956, 0)\t1\n",
      "  (15957, 0)\t1\n",
      "  (15958, 0)\t1\n",
      "  (15959, 0)\t1\n",
      "  (15960, 0)\t1\n",
      "  (15961, 0)\t1\n",
      "  (15962, 0)\t1\n",
      "  (15963, 0)\t1\n",
      "  (15964, 0)\t1\n",
      "  (15965, 0)\t1\n",
      "  (15966, 0)\t1\n",
      "  (15967, 0)\t1\n",
      "  (15968, 0)\t1\n",
      "  (15969, 0)\t1\n",
      "  (15970, 0)\t1\n",
      "  (15971, 0)\t1\n",
      "  (15972, 0)\t1\n",
      "  (15973, 0)\t1\n",
      "  (15974, 0)\t1\n",
      "  (15975, 0)\t1\n",
      "  (15976, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "# column names\n",
    "with open('data/column_names.txt', 'r') as f:\n",
    "    column_names = [line.strip() for line in f]\n",
    "\n",
    "sparse_dat = sparse.load_npz(\"data/sparse_df.npz\")\n",
    "\n",
    "# Extract labels from the first column\n",
    "labels = sparse_dat[:, 0]\n",
    "\n",
    "# Create a list of column indices to keep\n",
    "to_keep = list(set(range(sparse_dat.shape[1])) - set([0]))\n",
    "\n",
    "# Extract the design matrix\n",
    "X = sparse_dat[:, to_keep]\n",
    "\n",
    "print(f'labels: \\n{labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design matrix: \n",
      "  (17173, 0)\t1\n",
      "  (17240, 0)\t2\n",
      "  (17164, 1)\t1\n",
      "  (17363, 1)\t1\n",
      "  (17448, 1)\t1\n",
      "  (17910, 1)\t1\n",
      "  (17914, 1)\t1\n",
      "  (17933, 1)\t2\n",
      "  (15801, 2)\t1\n",
      "  (15867, 2)\t1\n",
      "  (16217, 2)\t1\n",
      "  (17173, 2)\t3\n",
      "  (17189, 2)\t1\n",
      "  (17386, 2)\t1\n",
      "  (17765, 2)\t4\n",
      "  (17933, 3)\t2\n",
      "  (17325, 4)\t1\n",
      "  (17366, 4)\t1\n",
      "  (16271, 5)\t1\n",
      "  (16837, 5)\t1\n",
      "  (17933, 6)\t2\n",
      "  (17933, 7)\t2\n",
      "  (17173, 8)\t2\n",
      "  (17933, 8)\t1\n",
      "  (15801, 9)\t1\n",
      "  :\t:\n",
      "  (8386, 56199)\t2\n",
      "  (8669, 56200)\t1\n",
      "  (10316, 56200)\t1\n",
      "  (8805, 56201)\t1\n",
      "  (9218, 56201)\t1\n",
      "  (9565, 56202)\t1\n",
      "  (9814, 56202)\t1\n",
      "  (6232, 56203)\t1\n",
      "  (9353, 56203)\t1\n",
      "  (5661, 56204)\t9\n",
      "  (6860, 56204)\t1\n",
      "  (8698, 56204)\t1\n",
      "  (125, 56205)\t1\n",
      "  (1228, 56205)\t1\n",
      "  (4422, 56205)\t1\n",
      "  (5648, 56206)\t1\n",
      "  (7464, 56206)\t1\n",
      "  (5648, 56207)\t2\n",
      "  (6025, 56208)\t2\n",
      "  (6025, 56209)\t2\n",
      "  (6151, 56210)\t2\n",
      "  (8669, 56211)\t1\n",
      "  (10316, 56211)\t1\n",
      "  (6340, 56212)\t1\n",
      "  (8696, 56212)\t1\n"
     ]
    }
   ],
   "source": [
    "print(f'Design matrix: \\n{X}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test split\n",
    "To do: consider stratifying by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14524, 56213)\n",
      "(14524, 1)\n",
      "(3632, 56213)\n",
      "(3632, 1)\n",
      "int64\n",
      "proportion of spam in training data: 0.39568989259157256\n",
      "proportion of spam in testing data: 0.11921806167400881\n"
     ]
    }
   ],
   "source": [
    "# To do - stratify the split \n",
    "n_samples = labels.shape[0]\n",
    "# Use train_test_split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.dtype)\n",
    "print(\"proportion of spam in training data:\", (y_train == 1).sum().item() / y_train.shape[0])\n",
    "print(\"proportion of spam in testing data:\", (y_test == 1).sum().item() / y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(y, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the proportion of type 2 errors - when the true label is 1 - spam, and the predicted label is 0 - ham\n",
    "\n",
    "        Args:\n",
    "        y: true labels\n",
    "        y_pred: predicted labels\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        type2errors = ((y == 1) & (y_pred == 0)).sum().item()\n",
    "        type1errors = ((y == 0) & (y_pred == 1)).sum().item()\n",
    "        correct = (y_pred == y).sum().item()\n",
    "        return type2errors, type1errors, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test models\n",
    "\n",
    "2 models:\n",
    "1. Without penalty\n",
    "2. With penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run AdaBoostWeak.py\n",
    "\n",
    "aboost1 = AdaBoostWeak(type2penalty = False, rounds = 200, maxDTdepth = 5)\n",
    "aboost1.fit(X = X_train, y = y_train.toarray().ravel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Test Accuracy:  0.6222466960352423\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 8 \n",
      " type 1 errors: 1364\n",
      "[1.738078422686433, 1.1578772472886218, 0.670030453820361, 0.7881697439875254, 0.4018609691686748, 0.7307869745209322, 0.6056352062561795, 0.5039509392007954, 0.4566812695620465, 0.44703549698409745, 0.42121387903947993, 0.4364816390206816, 0.4434868817547057, 0.34417171919447914, 0.37299717539730065, 0.35279121445788303, 0.42834644827723467, 0.3362065793329317, 0.3271012928109181, 0.2947063867289413, 0.2601273589714025, 0.2329995436063459, 0.2706815519172123, 0.29078431848943437, 0.23768522419635785, 0.17803592282954409, 0.2175952200101182, 0.23261007216875126, 0.3303683941887554, 0.27645168427950634, 0.22099460959922654, 0.23421590975545464, 0.21424062167925612, 0.20469388591430593, 0.21672864514179604, 0.2109742183013699, 0.32533254680201606, 0.29097497907465014, 0.21369171457340785, 0.2566234075229289, 0.22751208339947077, 0.19908104628957038, 0.26472643062253964, 0.1945340079843263, 0.1856476212017604, 0.18613771658735992, 0.18664307257477758, 0.201173345051725, 0.17775953581905732, 0.2933125502638943, 0.31370038914658266, 0.2935736769153734, 0.2129610393437167, 0.21604580367791482, 0.20419469437124127, 0.2097827111678369, 0.19371707435169555, 0.19112593689219534, 0.16952685653480884, 0.19546425200966644, 0.25207498274227635, 0.21363608572222348, 0.20467035451492052, 0.3552661737575845, 0.23366383485797684, 0.21635699963817773, 0.1709084602237629, 0.1057020811497346, 0.1294511986659167, 0.16782201403377786, 0.20920472925418945, 0.18594722833851723, 0.17099456480330752, 0.19892239475682214, 0.15889564341900217, 0.1365878503581026, 0.1600782602212827, 0.14358858768166508, 0.13758826698789575, 0.1539004628714384, 0.18968580815829475, 0.13218703680201385, 0.15715666514102608, 0.1938417986165794, 0.1397154318438462, 0.14979561034768493, 0.14979799967422305, 0.1254014534500534, 0.1644613755961366, 0.13283849865115377, 0.16242049656195262, 0.18229246754067224, 0.134818142715477, 0.18280696204014704, 0.19554962771896903, 0.12265314111317417, 0.13272836743284241, 0.1435172085950674, 0.12424262008414706, 0.11632959707183847, 0.10192395222956152, 0.09808834120883478, 0.09999855067891925, 0.07541537792115316, 0.10252712681134658, 0.11792076965970726, 0.1384558513894489, 0.1636121857130692, 0.12991392303395743, 0.11431812613976428, 0.14497383462892355, 0.19851663332684072, 0.1502693389564547, 0.11261367617046782, 0.12829021359518733, 0.09263980632086118, 0.10442328910785165, 0.324377699276929, 0.34070931561668955, 0.062033238047563465, 0.16583831844764035, 0.2218646271714322, 0.1462823018188959, 0.18181170470832309, 0.15679204486334705, 0.2023138770840245, 0.22040907428756207, 0.13735411557165217, 0.13206513550939875, 0.14142895609383632, 0.18378846042558383, 0.17296043108760928, 0.22564241011329964, 0.1912662292294203, 0.18572614051577924, 0.18177493834270106, 0.1637392104064465, 0.1338715894095958, 0.1664893288220461, 0.1679834699681248, 0.13342892577960686, 0.09275833392746541, 0.07110911791094739, 0.04756219337018695, 0.07509569643858119, 0.08355099832964323, 0.07551188085383015, 0.08896069381499845, 0.08474560725468541, 0.09253606758150996, 0.1128622918498017, 0.08536915232521841, 0.08372690023712077, 0.11556691814544963, 0.13534364244277028, 0.15389661989398506, 0.1242843912576306, 0.12419656486977017, 0.12424797471609116, 0.09772732725991969, 0.11829385957676683, 0.13776471486768765, 0.1187953876024613, 0.05794467730444605, 0.08036197016488596, 0.12555710962222416, 0.10745461274585906, 0.0933334049167582, 0.10100186288477689, 0.0969996536544603, 0.16002287845759666, 0.1672721773035433, 0.0904845118315149, 0.09753040928203714, 0.10943750470435276, 0.06092552547602307, 0.10170091803462059, 0.12737493948073766, 0.09169528182624151, 0.05300255313656429, 0.06695875902462049, 0.15554843596682544, 0.1333454518992507, 0.06746226666066311, 0.08928759458988704, 0.07688826720367108, 0.09115644027766147, 0.11825918749043174, 0.11252724177652511, 0.11878855994942128, 0.10106693575940848, 0.08686868944890792, 0.08382793143626499, 0.06517137266712822, 0.14721447329351695, 0.0938736990524432, 0.07092930989026147, 0.08576536668867647, 0.10492646900873075, 0.09660098432542208]\n",
      "[0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost1.predict(X_test)\n",
    "type2, type1, correct = errors(y_test.toarray().ravel(), predictions)\n",
    "print(\"Model 1 Test Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')\n",
    "print(aboost1.alphas)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (no penalty) Training set Accuracy:  0.981410079867805\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 143 \n",
      " type 1 errors: 127\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost1.predict(X_train)\n",
    "type2, type1, correct = errors(y_train.toarray().ravel(), predictions)\n",
    "print(\"Model 1 (no penalty) Training set Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning/Hyperparameter search\n",
    "\n",
    "Sigh. So much overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'type2penalty': [False] ,'rounds': [200, 300, 400, 500], 'maxDTdepth': [5,6,7,8,9,10]},\n",
    "    {'type2penalty': [True] ,'rounds': [200, 300, 400, 500], 'maxDTdepth': [5,6,7,8,9,10]}\n",
    "    ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
