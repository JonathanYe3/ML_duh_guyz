{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take 2: Adaboost with Weak Classifiers\n",
    "None of that bagging classifier bullshit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import guys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "# from AdaBoostClassifier import AdaBoostClassifier\n",
    "#%run AdaBoostWeak.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: \n",
      "  (2, 0)\t1\n",
      "  (5, 0)\t1\n",
      "  (8, 0)\t1\n",
      "  (9, 0)\t1\n",
      "  (11, 0)\t1\n",
      "  (12, 0)\t1\n",
      "  (15, 0)\t1\n",
      "  (19, 0)\t1\n",
      "  (34, 0)\t1\n",
      "  (42, 0)\t1\n",
      "  (54, 0)\t1\n",
      "  (56, 0)\t1\n",
      "  (65, 0)\t1\n",
      "  (67, 0)\t1\n",
      "  (68, 0)\t1\n",
      "  (93, 0)\t1\n",
      "  (95, 0)\t1\n",
      "  (114, 0)\t1\n",
      "  (117, 0)\t1\n",
      "  (120, 0)\t1\n",
      "  (121, 0)\t1\n",
      "  (123, 0)\t1\n",
      "  (134, 0)\t1\n",
      "  (135, 0)\t1\n",
      "  (139, 0)\t1\n",
      "  :\t:\n",
      "  (15952, 0)\t1\n",
      "  (15953, 0)\t1\n",
      "  (15954, 0)\t1\n",
      "  (15955, 0)\t1\n",
      "  (15956, 0)\t1\n",
      "  (15957, 0)\t1\n",
      "  (15958, 0)\t1\n",
      "  (15959, 0)\t1\n",
      "  (15960, 0)\t1\n",
      "  (15961, 0)\t1\n",
      "  (15962, 0)\t1\n",
      "  (15963, 0)\t1\n",
      "  (15964, 0)\t1\n",
      "  (15965, 0)\t1\n",
      "  (15966, 0)\t1\n",
      "  (15967, 0)\t1\n",
      "  (15968, 0)\t1\n",
      "  (15969, 0)\t1\n",
      "  (15970, 0)\t1\n",
      "  (15971, 0)\t1\n",
      "  (15972, 0)\t1\n",
      "  (15973, 0)\t1\n",
      "  (15974, 0)\t1\n",
      "  (15975, 0)\t1\n",
      "  (15976, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "# column names\n",
    "with open('data/column_names.txt', 'r') as f:\n",
    "    column_names = [line.strip() for line in f]\n",
    "\n",
    "sparse_dat = sparse.load_npz(\"data/sparse_df.npz\")\n",
    "\n",
    "# Extract labels from the first column\n",
    "labels = sparse_dat[:, 0]\n",
    "\n",
    "# Create a list of column indices to keep\n",
    "to_keep = list(set(range(sparse_dat.shape[1])) - set([0]))\n",
    "\n",
    "# Extract the design matrix\n",
    "X = sparse_dat[:, to_keep]\n",
    "\n",
    "print(f'labels: \\n{labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design matrix: \n",
      "  (17173, 0)\t1\n",
      "  (17240, 0)\t2\n",
      "  (17164, 1)\t1\n",
      "  (17363, 1)\t1\n",
      "  (17448, 1)\t1\n",
      "  (17910, 1)\t1\n",
      "  (17914, 1)\t1\n",
      "  (17933, 1)\t2\n",
      "  (15801, 2)\t1\n",
      "  (15867, 2)\t1\n",
      "  (16217, 2)\t1\n",
      "  (17173, 2)\t3\n",
      "  (17189, 2)\t1\n",
      "  (17386, 2)\t1\n",
      "  (17765, 2)\t4\n",
      "  (17933, 3)\t2\n",
      "  (17325, 4)\t1\n",
      "  (17366, 4)\t1\n",
      "  (16271, 5)\t1\n",
      "  (16837, 5)\t1\n",
      "  (17933, 6)\t2\n",
      "  (17933, 7)\t2\n",
      "  (17173, 8)\t2\n",
      "  (17933, 8)\t1\n",
      "  (15801, 9)\t1\n",
      "  :\t:\n",
      "  (8386, 56199)\t2\n",
      "  (8669, 56200)\t1\n",
      "  (10316, 56200)\t1\n",
      "  (8805, 56201)\t1\n",
      "  (9218, 56201)\t1\n",
      "  (9565, 56202)\t1\n",
      "  (9814, 56202)\t1\n",
      "  (6232, 56203)\t1\n",
      "  (9353, 56203)\t1\n",
      "  (5661, 56204)\t9\n",
      "  (6860, 56204)\t1\n",
      "  (8698, 56204)\t1\n",
      "  (125, 56205)\t1\n",
      "  (1228, 56205)\t1\n",
      "  (4422, 56205)\t1\n",
      "  (5648, 56206)\t1\n",
      "  (7464, 56206)\t1\n",
      "  (5648, 56207)\t2\n",
      "  (6025, 56208)\t2\n",
      "  (6025, 56209)\t2\n",
      "  (6151, 56210)\t2\n",
      "  (8669, 56211)\t1\n",
      "  (10316, 56211)\t1\n",
      "  (6340, 56212)\t1\n",
      "  (8696, 56212)\t1\n"
     ]
    }
   ],
   "source": [
    "print(f'Design matrix: \\n{X}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test split\n",
    "To do: consider stratifying by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14524, 56213)\n",
      "(14524, 1)\n",
      "(3632, 56213)\n",
      "(3632, 1)\n",
      "int64\n",
      "proportion of spam in training data: 0.39568989259157256\n",
      "proportion of spam in testing data: 0.11921806167400881\n"
     ]
    }
   ],
   "source": [
    "# To do - stratify the split \n",
    "n_samples = labels.shape[0]\n",
    "# Use train_test_split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.dtype)\n",
    "print(\"proportion of spam in training data:\", (y_train == 1).sum().item() / y_train.shape[0])\n",
    "print(\"proportion of spam in testing data:\", (y_test == 1).sum().item() / y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(y, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the proportion of type 2 errors - when the true label is 1 - spam, and the predicted label is 0 - ham\n",
    "\n",
    "        Args:\n",
    "        y: true labels\n",
    "        y_pred: predicted labels\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        type2errors = ((y == 1) & (y_pred == 0)).sum().item()\n",
    "        type1errors = ((y == 0) & (y_pred == 1)).sum().item()\n",
    "        correct = (y_pred == y).sum().item()\n",
    "        return type2errors, type1errors, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test models\n",
    "\n",
    "2 models:\n",
    "1. Without penalty\n",
    "2. With penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run AdaBoostWeak.py\n",
    "\n",
    "aboost1 = AdaBoostWeak(type2penalty = False, rounds = 400)\n",
    "aboost1.fit(X = X_train, y = y_train.toarray().ravel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Test Accuracy:  0.4611784140969163\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 4 \n",
      " type 1 errors: 1953\n",
      "[0.7382096923357286, 0.6907582821931111, 0.5850517698634371, 0.7437801392441605, 0.5022685316964053, 0.2928315041451402, 0.2549640030999755, 0.40728412974514483, 0.3271842574820191, 0.3737226583301492, 0.3309911109930502, 0.355777979218984, 0.24087567461131448, 0.20625485114509307, 0.18524759275358518, 0.2593192142703875, 0.4033100284579448, 0.2951991395994785, 0.2835448380995124, 0.27060121694525996, 0.13170692969374043, 0.14213103800037788, 0.2658234318367424, 0.264891537715884, 0.2627803155945294, 0.22910857126434245, 0.10445434449603595, 0.11195264689635728, 0.10580361919558777, 0.17004809483215408, 0.28484536899184215, 0.2187487753340198, 0.20686964301312677, 0.19364400408633242, 0.22667318659758232, 0.20549041566389262, 0.1993279282719188, 0.1901115039932025, 0.2057028606809913, 0.20608427905772111, 0.19273394223775453, 0.18074138819735555, 0.08229148441992035, 0.11401847019171317, 0.17957294524028083, 0.15063632554236653, 0.19349991730163216, 0.18115148978276696, 0.15562350873886302, 0.18603073595011665, 0.1741603232113636, 0.14674041165991622, 0.18505041873941414, 0.16522917602472717, 0.262663639374321, 0.10739128802910886, 0.11340480563548383, 0.1964368041072317, 0.1731876645894145, 0.08391134286551169, 0.10889060449916707, 0.10101674315564892, 0.0808014045809722, 0.07752489098061251, 0.13738288159645992, 0.21450566183652314, 0.15868614938629588, 0.15725746922153463, 0.16771935342066197, 0.15519388835837986, 0.14411747309963244, 0.0703228453807179, 0.07476785137531211, 0.07182050730143315, 0.08673033972058249, 0.08207921168445266, 0.07129821416040848, 0.14469867468388098, 0.13961248696390102, 0.12542958702042947, 0.12129876968660756, 0.13532212417508252, 0.13446531414971977, 0.16039968141747127, 0.14767404381281107, 0.12566757099380524, 0.12468507808859118, 0.11800955345033531, 0.14784454405030642, 0.14379631028301273, 0.1372900512281431, 0.2650351077276596, 0.1861563963430601, 0.16357404206328932, 0.12137580884335891, 0.10616220604468858, 0.1074644545932114, 0.11015120205178242, 0.11720598914646745, 0.11433712687132909, 0.129429241608155, 0.1372953276981243, 0.15295985539526044, 0.1354821463380342, 0.10411655766226993, 0.0890871469973881, 0.1827502957881208, 0.10339602835791409, 0.10506204034683203, 0.10796081734541402, 0.11083869890275326, 0.1087338164360878, 0.09902162255812268, 0.09848395850990106, 0.04976584468484319, 0.0738866298420451, 0.11629282387337939, 0.11442927891151766, 0.13591386170254538, 0.12873614116992277, 0.06281168353960735, 0.1953002937339414, 0.12467322223939102, 0.05533823317478541, 0.06424443274777653, 0.12959826615747702, 0.10949955722813845, 0.04562881899991582, 0.050889122715064024, 0.09860302804554899, 0.0928432822631987, 0.11157202051455534, 0.10831310305542867, 0.044224801458253915, 0.06899378652523634, 0.10999775748044083, 0.10060916484588149, 0.09916736583607128, 0.12192388112877116, 0.2298740180786758, 0.06426882856865232, 0.20545186727337816, 0.1365409597932921, 0.1367276390914392, 0.15560907782910785, 0.138143610537093, 0.12145110077321768, 0.1049138314487861, 0.13280613963626886, 0.12618910666991773, 0.10638435086445323, 0.10694262811255471, 0.10271759329680324, 0.09872503895304807, 0.10075459570482048, 0.0964633259488972, 0.08848796534269925, 0.0915920473619342, 0.09733189694417785, 0.09446714407368788, 0.12985443171941358, 0.12497262970119925, 0.10145308323258033, 0.1255108543759629, 0.11179104158408476, 0.04360600326748499, 0.0541930159702347, 0.09337758152503499, 0.08168089736438927, 0.04088285459869974, 0.06597607114588265, 0.10336864388926166, 0.09480242928481618, 0.052693307741678325, 0.07643530593588292, 0.12096520002125223, 0.08838335669406241, 0.04063252733905803, 0.04311675940396073, 0.042206713060704705, 0.04547592463717749, 0.044397630705704724, 0.04337811929442944, 0.042437000924673444, 0.05091505190078765, 0.12636751390879578, 0.1099707479192147, 0.0406665997923488, 0.06739516500447455, 0.1024513958345764, 0.0992762048718818, 0.05768129009528174, 0.07275318189259303, 0.11063457977783314, 0.08106323686117069, 0.09381918416604616, 0.13938396170876713, 0.11248487461585904, 0.08496268261353573, 0.10532316145713479, 0.0949151368826247, 0.07688168862618368, 0.08264596285867092, 0.08577936538201952, 0.07973202711018475, 0.0831135022523009, 0.08245593714730876, 0.03773847276876515, 0.06263766864289648, 0.05959441897070892, 0.055295462668266335, 0.09778643575851113, 0.08050532323162649, 0.11951369656310855, 0.1590017545336863, 0.08331111951443192, 0.045270515511186354, 0.07238956350350728, 0.10555565818925965, 0.07485395234997852, 0.0374862122870708, 0.04276238236798978, 0.04178638955408059, 0.04347421939120236, 0.04244803809436121, 0.04184985991266627, 0.040926987377500154, 0.03995666979146969, 0.10833290450557871, 0.10468296551857033, 0.04037208566368715, 0.06779312356457422, 0.0996996587487775, 0.0920563499008596, 0.0537888776030335, 0.06622905458055944, 0.11812480695151097, 0.09958225589939722, 0.08471687677620045, 0.08349192224464934, 0.0972509121496969, 0.08610469913964608, 0.035285557801492264, 0.04083266115490694, 0.03993590689558064, 0.038273504959827065, 0.03753128260143366, 0.05053334593858001, 0.08360549335864445, 0.06955471048631241, 0.034942387438443266, 0.036637814391114625, 0.03597865239387395, 0.03662620458404585, 0.03596745656058932, 0.04771125316533932, 0.04624520187429222, 0.03845544408847805, 0.037686843350087935, 0.04474134281291824, 0.08460131802855794, 0.0830481286561192, 0.041792417743855136, 0.03932389357843705, 0.03850090478941643, 0.05913726205910947, 0.0909098409510654, 0.0697977020738495, 0.07404844268651234, 0.07242643542078318, 0.0701558220073261, 0.06956874855983453, 0.03389266061237745, 0.09956967677350355, 0.06724587787975912, 0.03373933163598523, 0.04720034604156639, 0.08834394862328214, 0.08489497372533067, 0.07571605098552377, 0.06803811797946394, 0.064671455206082, 0.1401182837414693, 0.07952329111948356, 0.07137259257232303, 0.07372128312644305, 0.06954520832618737, 0.06846849913936336, 0.03505587480113742, 0.03657341532580457, 0.038916901539571155, 0.11160024238315239, 0.08575177409533921, 0.04574018210409653, 0.04294368972862895, 0.08372710874142547, 0.07627225302765293, 0.09102863995202827, 0.08591285576611231, 0.03297510586031738, 0.03846818634997576, 0.07058395094211306, 0.07164123694977498, 0.03817920750630625, 0.039940603077995795, 0.0741664325734037, 0.07296464190421453, 0.0757432619289718, 0.06967934430274483, 0.07302482664805472, 0.0828435485315902, 0.0778450756153147, 0.0676455925373523, 0.06342253920066863, 0.06344041485456325, 0.03226630043675369, 0.09325391639762069, 0.0664837907900724, 0.03529754493612642, 0.056616223580062415, 0.0903588324499073, 0.07516047212281926, 0.0849117038753875, 0.07666072173131533, 0.03204205461499484, 0.04700509655006468, 0.08086349092439896, 0.07885057925547839, 0.042569908497534566, 0.062350112431918105, 0.09101023864588635, 0.06288398110153802, 0.031054967996831332, 0.04174439244224254, 0.04064201885452381, 0.059639139649301856, 0.09759250950724031, 0.07148742559373784, 0.03180024115930966, 0.06071850719729205, 0.12627400778312095, 0.08946660534821431, 0.038282505558153775, 0.0965696177258143, 0.0803754909752136, 0.08250242707087835, 0.0661802747986676, 0.03188221552769455, 0.0372403717326569, 0.0364487988745824, 0.053959608733238754, 0.08155047902345294, 0.062259141939080574, 0.03234312712623599, 0.0504263419524584, 0.07851234240798267, 0.06470878451500797, 0.03445199745334224, 0.043356945898559394, 0.07598909403088758, 0.06937385713795616, 0.06602662552997936, 0.0641628176787806, 0.033321453160525365, 0.037848386651223496, 0.036992159268121175, 0.04098802870008592, 0.03989424074213353, 0.03262345914313423, 0.03207353523811064, 0.03311318744757276, 0.032536326707218996, 0.03695731844853609, 0.07151468203451095, 0.06802859392999404, 0.03286459845839497, 0.04252520330979598, 0.08090493947263046, 0.06753998750170069, 0.0296790614292824, 0.08898235496008668, 0.06349765703117939, 0.07866257028420459, 0.08234588445937753, 0.0366576894872738, 0.04913111223897638, 0.08051282431976094, 0.06273256516679679, 0.07878514261466531, 0.09699720218682842, 0.1814882686062655, 0.05611822733619862, 0.14397628524284645, 0.11758208405658858, 0.1173524574916244, 0.10333910720018162, 0.04480589265541597, 0.0709624813828207, 0.11375620681483734, 0.11346515572200622]\n",
      "[0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost1.predict(X_test)\n",
    "type2, type1, correct = errors(y_test.toarray().ravel(), predictions)\n",
    "print(\"Model 1 Test Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')\n",
    "print(aboost1.alphas)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (no penalty) Training set Accuracy:  0.9319746626273754\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 505 \n",
      " type 1 errors: 483\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost1.predict(X_train)\n",
    "type2, type1, correct = errors(y_train.toarray().ravel(), predictions)\n",
    "print(\"Model 1 (no penalty) Training set Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
