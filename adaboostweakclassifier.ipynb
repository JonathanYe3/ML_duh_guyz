{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take 2: Adaboost with Weak Classifiers\n",
    "None of that bagging classifier bullshit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import guys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "# from AdaBoostClassifier import AdaBoostClassifier\n",
    "#%run AdaBoostWeak.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: \n",
      "  (2, 0)\t1\n",
      "  (5, 0)\t1\n",
      "  (8, 0)\t1\n",
      "  (9, 0)\t1\n",
      "  (11, 0)\t1\n",
      "  (12, 0)\t1\n",
      "  (15, 0)\t1\n",
      "  (19, 0)\t1\n",
      "  (34, 0)\t1\n",
      "  (42, 0)\t1\n",
      "  (54, 0)\t1\n",
      "  (56, 0)\t1\n",
      "  (65, 0)\t1\n",
      "  (67, 0)\t1\n",
      "  (68, 0)\t1\n",
      "  (93, 0)\t1\n",
      "  (95, 0)\t1\n",
      "  (114, 0)\t1\n",
      "  (117, 0)\t1\n",
      "  (120, 0)\t1\n",
      "  (121, 0)\t1\n",
      "  (123, 0)\t1\n",
      "  (134, 0)\t1\n",
      "  (135, 0)\t1\n",
      "  (139, 0)\t1\n",
      "  :\t:\n",
      "  (15952, 0)\t1\n",
      "  (15953, 0)\t1\n",
      "  (15954, 0)\t1\n",
      "  (15955, 0)\t1\n",
      "  (15956, 0)\t1\n",
      "  (15957, 0)\t1\n",
      "  (15958, 0)\t1\n",
      "  (15959, 0)\t1\n",
      "  (15960, 0)\t1\n",
      "  (15961, 0)\t1\n",
      "  (15962, 0)\t1\n",
      "  (15963, 0)\t1\n",
      "  (15964, 0)\t1\n",
      "  (15965, 0)\t1\n",
      "  (15966, 0)\t1\n",
      "  (15967, 0)\t1\n",
      "  (15968, 0)\t1\n",
      "  (15969, 0)\t1\n",
      "  (15970, 0)\t1\n",
      "  (15971, 0)\t1\n",
      "  (15972, 0)\t1\n",
      "  (15973, 0)\t1\n",
      "  (15974, 0)\t1\n",
      "  (15975, 0)\t1\n",
      "  (15976, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "# column names\n",
    "with open('data/column_names.txt', 'r') as f:\n",
    "    column_names = [line.strip() for line in f]\n",
    "\n",
    "sparse_dat = sparse.load_npz(\"data/sparse_df.npz\")\n",
    "\n",
    "# Extract labels from the first column\n",
    "labels = sparse_dat[:, 0]\n",
    "\n",
    "# Create a list of column indices to keep\n",
    "to_keep = list(set(range(sparse_dat.shape[1])) - set([0]))\n",
    "\n",
    "# Extract the design matrix\n",
    "X = sparse_dat[:, to_keep]\n",
    "\n",
    "print(f'labels: \\n{labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design matrix: \n",
      "  (17173, 0)\t1\n",
      "  (17240, 0)\t2\n",
      "  (17164, 1)\t1\n",
      "  (17363, 1)\t1\n",
      "  (17448, 1)\t1\n",
      "  (17910, 1)\t1\n",
      "  (17914, 1)\t1\n",
      "  (17933, 1)\t2\n",
      "  (15801, 2)\t1\n",
      "  (15867, 2)\t1\n",
      "  (16217, 2)\t1\n",
      "  (17173, 2)\t3\n",
      "  (17189, 2)\t1\n",
      "  (17386, 2)\t1\n",
      "  (17765, 2)\t4\n",
      "  (17933, 3)\t2\n",
      "  (17325, 4)\t1\n",
      "  (17366, 4)\t1\n",
      "  (16271, 5)\t1\n",
      "  (16837, 5)\t1\n",
      "  (17933, 6)\t2\n",
      "  (17933, 7)\t2\n",
      "  (17173, 8)\t2\n",
      "  (17933, 8)\t1\n",
      "  (15801, 9)\t1\n",
      "  :\t:\n",
      "  (8386, 56199)\t2\n",
      "  (8669, 56200)\t1\n",
      "  (10316, 56200)\t1\n",
      "  (8805, 56201)\t1\n",
      "  (9218, 56201)\t1\n",
      "  (9565, 56202)\t1\n",
      "  (9814, 56202)\t1\n",
      "  (6232, 56203)\t1\n",
      "  (9353, 56203)\t1\n",
      "  (5661, 56204)\t9\n",
      "  (6860, 56204)\t1\n",
      "  (8698, 56204)\t1\n",
      "  (125, 56205)\t1\n",
      "  (1228, 56205)\t1\n",
      "  (4422, 56205)\t1\n",
      "  (5648, 56206)\t1\n",
      "  (7464, 56206)\t1\n",
      "  (5648, 56207)\t2\n",
      "  (6025, 56208)\t2\n",
      "  (6025, 56209)\t2\n",
      "  (6151, 56210)\t2\n",
      "  (8669, 56211)\t1\n",
      "  (10316, 56211)\t1\n",
      "  (6340, 56212)\t1\n",
      "  (8696, 56212)\t1\n"
     ]
    }
   ],
   "source": [
    "print(f'Design matrix: \\n{X}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test split\n",
    "To do: consider stratifying by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14524, 56213)\n",
      "(14524, 1)\n",
      "(3632, 56213)\n",
      "(3632, 1)\n",
      "int64\n",
      "proportion of spam in training data: 0.39568989259157256\n",
      "proportion of spam in testing data: 0.11921806167400881\n"
     ]
    }
   ],
   "source": [
    "# To do - stratify the split \n",
    "n_samples = labels.shape[0]\n",
    "# Use train_test_split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.dtype)\n",
    "print(\"proportion of spam in training data:\", (y_train == 1).sum().item() / y_train.shape[0])\n",
    "print(\"proportion of spam in testing data:\", (y_test == 1).sum().item() / y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(y, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the proportion of type 2 errors - when the true label is 1 - spam, and the predicted label is 0 - ham\n",
    "\n",
    "        Args:\n",
    "        y: true labels\n",
    "        y_pred: predicted labels\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        type2errors = ((y == 1) & (y_pred == 0)).sum().item()\n",
    "        type1errors = ((y == 0) & (y_pred == 1)).sum().item()\n",
    "        correct = (y_pred == y).sum().item()\n",
    "        return type2errors, type1errors, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test models\n",
    "\n",
    "2 models:\n",
    "1. Without penalty\n",
    "2. With penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run AdaBoostWeak.py\n",
    "\n",
    "aboost1 = AdaBoostWeak(type2penalty = False, rounds = 200, maxDTdepth = 5)\n",
    "aboost1.fit(X = X_train, y = y_train.toarray().ravel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Test Accuracy:  0.6057268722466961\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 9 \n",
      " type 1 errors: 1423\n",
      "[0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost1.predict(X_test)\n",
    "type2, type1, correct = errors(y_test.toarray().ravel(), predictions)\n",
    "print(\"Model 1 Test Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')\n",
    "# print(aboost1.alphas)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (no penalty) Training set Accuracy:  0.9816854860919857\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 149 \n",
      " type 1 errors: 117\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost1.predict(X_train)\n",
    "type2, type1, correct = errors(y_train.toarray().ravel(), predictions)\n",
    "print(\"Model 1 (no penalty) Training set Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run AdaBoostWeak.py\n",
    "\n",
    "aboost2 = AdaBoostWeak(type2penalty = True, rounds = 200, maxDTdepth = 5)\n",
    "aboost2.fit(X = X_train, y = y_train.toarray().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 (with penalty) Training set Accuracy:  0.9998622968879096\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 2 \n",
      " type 1 errors: 0\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost2.predict(X_train)\n",
    "type2, type1, correct = errors(y_train.toarray().ravel(), predictions)\n",
    "print(\"Model 2 (with penalty) Training set Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 Test Accuracy:  0.7285242290748899\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 37 \n",
      " type 1 errors: 949\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost2.predict(X_test)\n",
    "type2, type1, correct = errors(y_test.toarray().ravel(), predictions)\n",
    "print(\"Model 2 Test Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')\n",
    "# print(aboost1.alphas)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run AdaBoostWeakClassic.py\n",
    "\n",
    "aboost3 = AdaBoostWeakClassic(type2penalty = False, rounds = 200, maxDTdepth = 5)\n",
    "aboost3.fit(X = X_train, y = y_train.toarray().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 (classic w/o penalty) Training set Accuracy:  0.993390250619664\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 2 \n",
      " type 1 errors: 94\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost3.predict(X_train)\n",
    "type2, type1, correct = errors(y_train.toarray().ravel(), predictions)\n",
    "print(\"Model 3 (classic w/o penalty) Training set Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 Test Accuracy:  0.6734581497797357\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 26 \n",
      " type 1 errors: 1160\n",
      "[0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost3.predict(X_test)\n",
    "type2, type1, correct = errors(y_test.toarray().ravel(), predictions)\n",
    "print(\"Model 3 Test Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')\n",
    "# print(aboost1.alphas)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run AdaBoostWeakClassic.py\n",
    "\n",
    "aboost4 = AdaBoostWeakClassic(type2penalty = True, rounds = 200, maxDTdepth = 5)\n",
    "aboost4.fit(X = X_train, y = y_train.toarray().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4 (classic with penalty) Training set Accuracy:  0.9956623519691545\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 0 \n",
      " type 1 errors: 63\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost4.predict(X_train)\n",
    "type2, type1, correct = errors(y_train.toarray().ravel(), predictions)\n",
    "print(\"Model 4 (classic with penalty) Training set Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4 Test Accuracy:  0.7409140969162996\n",
      "unique predictions - should be 0 and 1: [0 1]\n",
      "type 2 errors: 27 \n",
      " type 1 errors: 914\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = aboost4.predict(X_test)\n",
    "type2, type1, correct = errors(y_test.toarray().ravel(), predictions)\n",
    "print(\"Model 4 Test Accuracy: \", correct/len(predictions))\n",
    "print(f'unique predictions - should be 0 and 1: {np.unique(predictions)}')\n",
    "print(f'type 2 errors: {type2} \\n type 1 errors: {type1}')\n",
    "# print(aboost1.alphas)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning/Hyperparameter search\n",
    "\n",
    "Sigh. So much overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = \\\n",
    "{'type2penalty': [False, True] ,'rounds': [200, 300, 400, 500], 'maxDTdepth': [5,6,7,8,9,10], 'pen_factor': [0.1, 0.3, 0.5, 0.7, 1.2, 1.5, 1.7, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdaBoostWeakClassic.py\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m aboost \u001b[38;5;241m=\u001b[39m AdaBoostWeakClassic(type2penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, rounds \u001b[38;5;241m=\u001b[39m rounds, maxDTdepth \u001b[38;5;241m=\u001b[39m maxDTdepth, pen_factor\u001b[38;5;241m=\u001b[39mpen_factor)\n\u001b[1;32m----> 6\u001b[0m \u001b[43maboost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# training reports\u001b[39;00m\n\u001b[0;32m      9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m aboost4\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\SchoolFiles\\ML\\Project\\ML_duh_guyz\\AdaBoostWeakClassic.py:105\u001b[0m, in \u001b[0;36mAdaBoostWeakClassic.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# print(w_i)\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# (a) Fit weak classifier and predict labels\u001b[39;00m\n\u001b[0;32m    104\u001b[0m stump \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxDTdepth)     \u001b[38;5;66;03m# Stump: Two terminal-node classification tree\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m \u001b[43mstump\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m stump\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstumps\u001b[38;5;241m.\u001b[39mappend(stump) \u001b[38;5;66;03m# Save to list of weak classifiers\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\tree\\_classes.py:1009\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    980\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \n\u001b[0;32m    982\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NA = \"N/A\"\n",
    "for rounds in param_grid['rounds']:\n",
    "    for maxDTdepth in param_grid['maxDTdepth']:\n",
    "        %run AdaBoostWeakClassic.py\n",
    "        aboost = AdaBoostWeakClassic(type2penalty = False, rounds = rounds, maxDTdepth = maxDTdepth, pen_factor=pen_factor)\n",
    "        aboost.fit(X = X_train, y = y_train.toarray().ravel())\n",
    "\n",
    "        # training reports\n",
    "        predictions = aboost4.predict(X_train)\n",
    "        type2, type1, correct = errors(y_train.toarray().ravel(), predictions)\n",
    "        print(f\"Model without penalty | rounds: {rounds} | maxDTdepth: {maxDTdepth}\")\n",
    "        print(\"Training Accuracy: \", correct/len(predictions))\n",
    "        print(f'type 2 errors: {type2} \\ntype 1 errors: {type1}')\n",
    "\n",
    "        # testing reports\n",
    "        predictions = aboost4.predict(X_test)\n",
    "        type2, type1, correct = errors(y_test.toarray().ravel(), predictions)\n",
    "        print(\"Test Accuracy: \", correct/len(predictions))\n",
    "        print(f'type 2 errors: {type2} \\ntype 1 errors: {type1}\\n')\n",
    "        \n",
    "        for pen_factor in param_grid['pen_factor']:\n",
    "            %run AdaBoostWeakClassic.py\n",
    "            aboost = AdaBoostWeakClassic(type2penalty = type2penalty, rounds = rounds, maxDTdepth = maxDTdepth, pen_factor=pen_factor)\n",
    "            aboost.fit(X = X_train, y = y_train.toarray().ravel())\n",
    "\n",
    "            # training reports\n",
    "            predictions = aboost4.predict(X_train)\n",
    "            type2, type1, correct = errors(y_train.toarray().ravel(), predictions)\n",
    "            print(f\"Model with penalty: {type2penalty}, {pen_factor} | rounds: {rounds} | maxDTdepth: {maxDTdepth}\")\n",
    "            print(\"Training Accuracy: \", correct/len(predictions))\n",
    "            print(f'type 2 errors: {type2} \\ntype 1 errors: {type1}')\n",
    "\n",
    "            # testing reports\n",
    "            predictions = aboost4.predict(X_test)\n",
    "            type2, type1, correct = errors(y_test.toarray().ravel(), predictions)\n",
    "            print(\"Test Accuracy: \", correct/len(predictions))\n",
    "            print(f'type 2 errors: {type2} \\ntype 1 errors: {type1}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
